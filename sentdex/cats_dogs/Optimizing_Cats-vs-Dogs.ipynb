{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obvious things to tweak\n",
    "\n",
    "\n",
    "Number of layers\n",
    "Nodes per layer\n",
    "Dense layer at the end or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-conv-64-nodes1-dense-1540675743\n",
      "4-conv-128-nodes1-dense-1540675743\n",
      "4-conv-64-nodes2-dense-1540675743\n",
      "4-conv-128-nodes2-dense-1540675743\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "dense_layers = [1, 2]\n",
    "layer_sizes = [64, 128]\n",
    "conv_layers = [4]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22451 samples, validate on 2495 samples\n",
      "Epoch 1/10\n",
      "22451/22451 [==============================] - 17s 759us/step - loss: 0.6488 - acc: 0.6194 - val_loss: 0.6195 - val_acc: 0.6425\n",
      "Epoch 2/10\n",
      "22451/22451 [==============================] - 17s 745us/step - loss: 0.5812 - acc: 0.7025 - val_loss: 0.5884 - val_acc: 0.6910\n",
      "Epoch 3/10\n",
      "22451/22451 [==============================] - 17s 751us/step - loss: 0.5173 - acc: 0.7517 - val_loss: 0.4794 - val_acc: 0.7792\n",
      "Epoch 4/10\n",
      "22451/22451 [==============================] - 17s 751us/step - loss: 0.4756 - acc: 0.7782 - val_loss: 0.4830 - val_acc: 0.7671\n",
      "Epoch 5/10\n",
      "22451/22451 [==============================] - 17s 759us/step - loss: 0.4517 - acc: 0.7936 - val_loss: 0.4388 - val_acc: 0.7980\n",
      "Epoch 6/10\n",
      "22451/22451 [==============================] - 17s 749us/step - loss: 0.4280 - acc: 0.8061 - val_loss: 0.4574 - val_acc: 0.7820\n",
      "Epoch 7/10\n",
      "22451/22451 [==============================] - 17s 752us/step - loss: 0.4084 - acc: 0.8127 - val_loss: 0.4266 - val_acc: 0.7996\n",
      "Epoch 8/10\n",
      "22451/22451 [==============================] - 17s 752us/step - loss: 0.3937 - acc: 0.8229 - val_loss: 0.4393 - val_acc: 0.7948\n",
      "Epoch 9/10\n",
      "22451/22451 [==============================] - 17s 754us/step - loss: 0.3757 - acc: 0.8310 - val_loss: 0.4046 - val_acc: 0.8184\n",
      "Epoch 10/10\n",
      "22451/22451 [==============================] - 17s 754us/step - loss: 0.3618 - acc: 0.8401 - val_loss: 0.4044 - val_acc: 0.8164\n",
      "Train on 22451 samples, validate on 2495 samples\n",
      "Epoch 1/10\n",
      "22451/22451 [==============================] - 27s 1ms/step - loss: 0.6461 - acc: 0.6217 - val_loss: 0.5948 - val_acc: 0.6922\n",
      "Epoch 2/10\n",
      "22451/22451 [==============================] - 27s 1ms/step - loss: 0.5756 - acc: 0.7088 - val_loss: 0.5417 - val_acc: 0.7259\n",
      "Epoch 3/10\n",
      "22451/22451 [==============================] - 27s 1ms/step - loss: 0.5249 - acc: 0.7439 - val_loss: 0.5035 - val_acc: 0.7563\n",
      "Epoch 4/10\n",
      "22451/22451 [==============================] - 27s 1ms/step - loss: 0.4857 - acc: 0.7708 - val_loss: 0.4751 - val_acc: 0.7715\n",
      "Epoch 5/10\n",
      "22451/22451 [==============================] - 27s 1ms/step - loss: 0.4572 - acc: 0.7911 - val_loss: 0.4604 - val_acc: 0.7812\n",
      "Epoch 6/10\n",
      "22451/22451 [==============================] - 27s 1ms/step - loss: 0.4328 - acc: 0.8014 - val_loss: 0.4652 - val_acc: 0.7868\n",
      "Epoch 7/10\n",
      "22451/22451 [==============================] - 27s 1ms/step - loss: 0.4108 - acc: 0.8119 - val_loss: 0.4412 - val_acc: 0.7972\n",
      "Epoch 8/10\n",
      "22451/22451 [==============================] - 27s 1ms/step - loss: 0.3863 - acc: 0.8266 - val_loss: 0.4581 - val_acc: 0.7852\n",
      "Epoch 9/10\n",
      "22451/22451 [==============================] - 27s 1ms/step - loss: 0.3725 - acc: 0.8335 - val_loss: 0.4431 - val_acc: 0.7968\n",
      "Epoch 10/10\n",
      "22451/22451 [==============================] - 27s 1ms/step - loss: 0.3522 - acc: 0.8437 - val_loss: 0.4400 - val_acc: 0.7992\n",
      "Train on 22451 samples, validate on 2495 samples\n",
      "Epoch 1/10\n",
      "22451/22451 [==============================] - 51s 2ms/step - loss: 0.6717 - acc: 0.5731 - val_loss: 0.6164 - val_acc: 0.6581\n",
      "Epoch 2/10\n",
      "22451/22451 [==============================] - 51s 2ms/step - loss: 0.6072 - acc: 0.6767 - val_loss: 0.5838 - val_acc: 0.6842\n",
      "Epoch 3/10\n",
      "22451/22451 [==============================] - 51s 2ms/step - loss: 0.5618 - acc: 0.7129 - val_loss: 0.5330 - val_acc: 0.7395\n",
      "Epoch 4/10\n",
      "22451/22451 [==============================] - 51s 2ms/step - loss: 0.5335 - acc: 0.7362 - val_loss: 0.5383 - val_acc: 0.7311\n",
      "Epoch 5/10\n",
      "22451/22451 [==============================] - 51s 2ms/step - loss: 0.5130 - acc: 0.7502 - val_loss: 0.5109 - val_acc: 0.7487\n",
      "Epoch 6/10\n",
      "22451/22451 [==============================] - 51s 2ms/step - loss: 0.4964 - acc: 0.7631 - val_loss: 0.5303 - val_acc: 0.7291\n",
      "Epoch 7/10\n",
      "22451/22451 [==============================] - 51s 2ms/step - loss: 0.4770 - acc: 0.7776 - val_loss: 0.4847 - val_acc: 0.7675\n",
      "Epoch 8/10\n",
      "22451/22451 [==============================] - 51s 2ms/step - loss: 0.4620 - acc: 0.7862 - val_loss: 0.4787 - val_acc: 0.7764\n",
      "Epoch 9/10\n",
      "22451/22451 [==============================] - 51s 2ms/step - loss: 0.4451 - acc: 0.7931 - val_loss: 0.5071 - val_acc: 0.7555\n",
      "Epoch 10/10\n",
      "22451/22451 [==============================] - 51s 2ms/step - loss: 0.4371 - acc: 0.7988 - val_loss: 0.5279 - val_acc: 0.7403\n",
      "Train on 22451 samples, validate on 2495 samples\n",
      "Epoch 1/10\n",
      "22451/22451 [==============================] - 17s 775us/step - loss: 0.6932 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.4866\n",
      "Epoch 2/10\n",
      "22451/22451 [==============================] - 17s 760us/step - loss: 0.6932 - acc: 0.4972 - val_loss: 0.6933 - val_acc: 0.4866\n",
      "Epoch 3/10\n",
      "22451/22451 [==============================] - 17s 765us/step - loss: 0.6932 - acc: 0.4974 - val_loss: 0.6933 - val_acc: 0.4866\n",
      "Epoch 4/10\n",
      "22451/22451 [==============================] - 17s 768us/step - loss: 0.6932 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4866\n",
      "Epoch 5/10\n",
      "22451/22451 [==============================] - 17s 778us/step - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6933 - val_acc: 0.4866\n",
      "Epoch 6/10\n",
      "22451/22451 [==============================] - 17s 773us/step - loss: 0.6931 - acc: 0.5019 - val_loss: 0.6935 - val_acc: 0.4866\n",
      "Epoch 7/10\n",
      "22451/22451 [==============================] - 17s 769us/step - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4866\n",
      "Epoch 8/10\n",
      "22451/22451 [==============================] - 17s 773us/step - loss: 0.6932 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4866\n",
      "Epoch 9/10\n",
      "22451/22451 [==============================] - 17s 771us/step - loss: 0.6932 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.4866\n",
      "Epoch 10/10\n",
      "22451/22451 [==============================] - 17s 778us/step - loss: 0.6932 - acc: 0.4973 - val_loss: 0.6932 - val_acc: 0.4866\n",
      "Train on 22451 samples, validate on 2495 samples\n",
      "Epoch 1/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.6402 - acc: 0.6335 - val_loss: 0.5985 - val_acc: 0.7022\n",
      "Epoch 2/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.5597 - acc: 0.7191 - val_loss: 0.5167 - val_acc: 0.7511\n",
      "Epoch 3/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.4974 - acc: 0.7616 - val_loss: 0.5101 - val_acc: 0.7511\n",
      "Epoch 4/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.4462 - acc: 0.7919 - val_loss: 0.4682 - val_acc: 0.7784\n",
      "Epoch 5/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.3974 - acc: 0.8188 - val_loss: 0.4596 - val_acc: 0.7768\n",
      "Epoch 6/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.3432 - acc: 0.8487 - val_loss: 0.4504 - val_acc: 0.7836\n",
      "Epoch 7/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.2888 - acc: 0.8769 - val_loss: 0.4896 - val_acc: 0.8024\n",
      "Epoch 8/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.2265 - acc: 0.9081 - val_loss: 0.5623 - val_acc: 0.7884\n",
      "Epoch 9/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.1649 - acc: 0.9345 - val_loss: 0.6749 - val_acc: 0.7824\n",
      "Epoch 10/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.1155 - acc: 0.9547 - val_loss: 0.7532 - val_acc: 0.7872\n",
      "Train on 22451 samples, validate on 2495 samples\n",
      "Epoch 1/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.6787 - acc: 0.5570 - val_loss: 0.6158 - val_acc: 0.6593\n",
      "Epoch 2/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.5893 - acc: 0.6855 - val_loss: 0.5401 - val_acc: 0.7251\n",
      "Epoch 3/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.5275 - acc: 0.7366 - val_loss: 0.5205 - val_acc: 0.7483\n",
      "Epoch 4/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.4790 - acc: 0.7733 - val_loss: 0.4993 - val_acc: 0.7583\n",
      "Epoch 5/10\n",
      "22451/22451 [==============================] - 53s 2ms/step - loss: 0.4176 - acc: 0.8066 - val_loss: 0.5284 - val_acc: 0.7511\n",
      "Epoch 6/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.3317 - acc: 0.8555 - val_loss: 0.5884 - val_acc: 0.7415\n",
      "Epoch 7/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.2325 - acc: 0.9066 - val_loss: 0.7003 - val_acc: 0.7479\n",
      "Epoch 8/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.1349 - acc: 0.9480 - val_loss: 0.9484 - val_acc: 0.7315\n",
      "Epoch 9/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.0763 - acc: 0.9723 - val_loss: 1.1931 - val_acc: 0.7230\n",
      "Epoch 10/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.0421 - acc: 0.9860 - val_loss: 1.3845 - val_acc: 0.7311\n",
      "Train on 22451 samples, validate on 2495 samples\n",
      "Epoch 1/10\n",
      "22451/22451 [==============================] - 18s 794us/step - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5134\n",
      "Epoch 2/10\n",
      "22451/22451 [==============================] - 17s 774us/step - loss: 0.6932 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5138\n",
      "Epoch 3/10\n",
      "22451/22451 [==============================] - 17s 768us/step - loss: 0.6932 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.4866\n",
      "Epoch 4/10\n",
      "22451/22451 [==============================] - 17s 772us/step - loss: 0.6933 - acc: 0.4950 - val_loss: 0.6933 - val_acc: 0.4866\n",
      "Epoch 5/10\n",
      "22451/22451 [==============================] - 17s 771us/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4866\n",
      "Epoch 6/10\n",
      "22451/22451 [==============================] - 17s 771us/step - loss: 0.6932 - acc: 0.5018 - val_loss: 0.6933 - val_acc: 0.4866\n",
      "Epoch 7/10\n",
      "22451/22451 [==============================] - 17s 771us/step - loss: 0.6932 - acc: 0.5043 - val_loss: 0.6930 - val_acc: 0.5134\n",
      "Epoch 8/10\n",
      "22451/22451 [==============================] - 17s 767us/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6933 - val_acc: 0.4866\n",
      "Epoch 9/10\n",
      "22451/22451 [==============================] - 17s 777us/step - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.4866\n",
      "Epoch 10/10\n",
      "22451/22451 [==============================] - 17s 778us/step - loss: 0.6932 - acc: 0.5023 - val_loss: 0.6933 - val_acc: 0.4866\n",
      "Train on 22451 samples, validate on 2495 samples\n",
      "Epoch 1/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.6742 - acc: 0.5601 - val_loss: 0.6188 - val_acc: 0.6581\n",
      "Epoch 2/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.6016 - acc: 0.6794 - val_loss: 0.5795 - val_acc: 0.6998\n",
      "Epoch 3/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.5517 - acc: 0.7188 - val_loss: 0.5279 - val_acc: 0.7315\n",
      "Epoch 4/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.5110 - acc: 0.7481 - val_loss: 0.5155 - val_acc: 0.7443\n",
      "Epoch 5/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.4707 - acc: 0.7744 - val_loss: 0.4946 - val_acc: 0.7547\n",
      "Epoch 6/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.4343 - acc: 0.7964 - val_loss: 0.5078 - val_acc: 0.7515\n",
      "Epoch 7/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.3949 - acc: 0.8204 - val_loss: 0.5192 - val_acc: 0.7627\n",
      "Epoch 8/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.3499 - acc: 0.8448 - val_loss: 0.5235 - val_acc: 0.7607\n",
      "Epoch 9/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.3018 - acc: 0.8656 - val_loss: 0.5637 - val_acc: 0.7627\n",
      "Epoch 10/10\n",
      "22451/22451 [==============================] - 28s 1ms/step - loss: 0.2508 - acc: 0.8951 - val_loss: 0.6514 - val_acc: 0.7515\n",
      "Train on 22451 samples, validate on 2495 samples\n",
      "Epoch 1/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.6605 - acc: 0.5868 - val_loss: 0.5976 - val_acc: 0.6818\n",
      "Epoch 2/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.5824 - acc: 0.6934 - val_loss: 0.5401 - val_acc: 0.7315\n",
      "Epoch 3/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.5127 - acc: 0.7493 - val_loss: 0.4985 - val_acc: 0.7531\n",
      "Epoch 4/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.4473 - acc: 0.7925 - val_loss: 0.4750 - val_acc: 0.7800\n",
      "Epoch 5/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.3716 - acc: 0.8340 - val_loss: 0.4927 - val_acc: 0.7643\n",
      "Epoch 6/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.2802 - acc: 0.8819 - val_loss: 0.5361 - val_acc: 0.7776\n",
      "Epoch 7/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.1889 - acc: 0.9241 - val_loss: 0.6877 - val_acc: 0.7571\n",
      "Epoch 8/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.1084 - acc: 0.9593 - val_loss: 1.0290 - val_acc: 0.7591\n",
      "Epoch 9/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.0765 - acc: 0.9713 - val_loss: 1.1858 - val_acc: 0.7655\n",
      "Epoch 10/10\n",
      "22451/22451 [==============================] - 54s 2ms/step - loss: 0.0592 - acc: 0.9800 - val_loss: 1.2120 - val_acc: 0.7663\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "# NOTE: To shrink the VRAM fraction add this to ConfigProto\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "# K.tf.ConfigProto(gpu_options=gpu_options)\n",
    "\n",
    "cfg = K.tf.ConfigProto(device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import normalize\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, rotation_range=10, width_shift_range=0.1,\n",
    "                             height_shift_range=0.1, shear_range=0.15,\n",
    "                             zoom_range=0.1, channel_shift_range=10., horizontal_flip=True)\n",
    "\n",
    "X = pickle.load(open(\"data/pickled/X.pickle\", \"rb\"))\n",
    "y = pickle.load(open(\"data/pickled/y.pickle\", \"rb\"))\n",
    "\n",
    "X = keras.utils.normalize(X, axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2)\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            K.clear_session()\n",
    "            NAME = \"{}-conv-{}-nodes{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            tensorboard = TensorBoard(log_dir='logs_r2/{}'.format(NAME))\n",
    "            \n",
    "            model = Sequential()\n",
    "\n",
    "            # the 128 can be anything\n",
    "            # 3,3 is the window of the convolution (3 pixels X 3 pixels)\n",
    "            # X.shape[1:] is 128, 128, 1 based ont he fact that I transformed the images to 128X128 pixes\n",
    "            model.add(Conv2D(layer_size, (3,3), input_shape = X.shape[1:]))\n",
    "            model.add(Activation(\"relu\"))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "            \n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(layer_size, (3,3)))\n",
    "                model.add(Activation(\"relu\"))\n",
    "                model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "            \n",
    "            model.add(Flatten())\n",
    "            for l in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation(\"relu\"))\n",
    "\n",
    "            # Output Layer\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                         optimizer='adam',\n",
    "                         metrics=['accuracy'])\n",
    "            \n",
    "            model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                               steps_per_epoch = len(X) / 32,\n",
    "                               epochs = 10,\n",
    "                               validation_data=(X_val, y_val),\n",
    "                               callbacks=[tensorboard])\n",
    "\n",
    "            model.fit(X, y, batch_size=32, epochs=10, validation_split=0.1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "780/779 [==============================] - 38s 48ms/step - loss: 0.6663 - acc: 0.5857 - val_loss: 5.6177 - val_acc: 0.6413\n",
      "Epoch 2/12\n",
      "780/779 [==============================] - 37s 47ms/step - loss: 0.5481 - acc: 0.7220 - val_loss: 4.0083 - val_acc: 0.7385\n",
      "Epoch 3/12\n",
      "780/779 [==============================] - 37s 47ms/step - loss: 0.4580 - acc: 0.7825 - val_loss: 3.7140 - val_acc: 0.7595\n",
      "Epoch 4/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.3981 - acc: 0.8181 - val_loss: 3.2478 - val_acc: 0.7878\n",
      "Epoch 5/12\n",
      "780/779 [==============================] - 37s 47ms/step - loss: 0.3580 - acc: 0.8388 - val_loss: 3.0616 - val_acc: 0.7994\n",
      "Epoch 6/12\n",
      "780/779 [==============================] - 37s 47ms/step - loss: 0.3303 - acc: 0.8550 - val_loss: 3.6416 - val_acc: 0.7613\n",
      "Epoch 7/12\n",
      "780/779 [==============================] - 37s 47ms/step - loss: 0.3077 - acc: 0.8660 - val_loss: 2.8708 - val_acc: 0.8152\n",
      "Epoch 8/12\n",
      "780/779 [==============================] - 37s 47ms/step - loss: 0.2867 - acc: 0.8756 - val_loss: 3.6194 - val_acc: 0.7663\n",
      "Epoch 9/12\n",
      "780/779 [==============================] - 37s 47ms/step - loss: 0.2680 - acc: 0.8875 - val_loss: 2.6135 - val_acc: 0.8293\n",
      "Epoch 10/12\n",
      "780/779 [==============================] - 37s 47ms/step - loss: 0.2616 - acc: 0.8897 - val_loss: 2.5291 - val_acc: 0.8337\n",
      "Epoch 11/12\n",
      "780/779 [==============================] - 37s 47ms/step - loss: 0.2394 - acc: 0.8981 - val_loss: 2.4516 - val_acc: 0.8379\n",
      "Epoch 12/12\n",
      "780/779 [==============================] - 37s 47ms/step - loss: 0.2283 - acc: 0.9050 - val_loss: 1.9832 - val_acc: 0.8713\n",
      "Epoch 1/12\n",
      "780/779 [==============================] - 81s 103ms/step - loss: 0.6815 - acc: 0.5589 - val_loss: 6.0997 - val_acc: 0.5832\n",
      "Epoch 2/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.6166 - acc: 0.6624 - val_loss: 6.3131 - val_acc: 0.5860\n",
      "Epoch 3/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.5281 - acc: 0.7341 - val_loss: 4.9026 - val_acc: 0.6850\n",
      "Epoch 4/12\n",
      "780/779 [==============================] - 80s 102ms/step - loss: 0.4475 - acc: 0.7911 - val_loss: 4.0720 - val_acc: 0.7355s: 0.4479 - acc: 0.7\n",
      "Epoch 5/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.3831 - acc: 0.8274 - val_loss: 3.7897 - val_acc: 0.7549\n",
      "Epoch 6/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.3329 - acc: 0.8528 - val_loss: 3.6189 - val_acc: 0.7677\n",
      "Epoch 7/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.3097 - acc: 0.8650 - val_loss: 4.0640 - val_acc: 0.7407\n",
      "Epoch 8/12\n",
      "780/779 [==============================] - 79s 102ms/step - loss: 0.2871 - acc: 0.8761 - val_loss: 3.0248 - val_acc: 0.8038\n",
      "Epoch 9/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.2578 - acc: 0.8925 - val_loss: 3.0146 - val_acc: 0.8076\n",
      "Epoch 10/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.2450 - acc: 0.8991 - val_loss: 2.4991 - val_acc: 0.8359\n",
      "Epoch 11/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.2354 - acc: 0.8997 - val_loss: 2.7212 - val_acc: 0.8206\n",
      "Epoch 12/12\n",
      "780/779 [==============================] - 80s 102ms/step - loss: 0.2233 - acc: 0.9057 - val_loss: 4.0549 - val_acc: 0.7365\n",
      "Epoch 1/12\n",
      "780/779 [==============================] - 38s 48ms/step - loss: 0.6730 - acc: 0.5734 - val_loss: 5.7117 - val_acc: 0.6202\n",
      "Epoch 2/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.5649 - acc: 0.7087 - val_loss: 5.1059 - val_acc: 0.6713\n",
      "Epoch 3/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.4489 - acc: 0.7910 - val_loss: 3.3356 - val_acc: 0.7752\n",
      "Epoch 4/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.3859 - acc: 0.8259 - val_loss: 3.6158 - val_acc: 0.7685\n",
      "Epoch 5/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.3429 - acc: 0.8490 - val_loss: 2.7781 - val_acc: 0.8176\n",
      "Epoch 6/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.3105 - acc: 0.8636 - val_loss: 2.5467 - val_acc: 0.8357\n",
      "Epoch 7/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.2867 - acc: 0.8772 - val_loss: 2.6455 - val_acc: 0.8253\n",
      "Epoch 8/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.2669 - acc: 0.8850 - val_loss: 2.8093 - val_acc: 0.8148\n",
      "Epoch 9/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.2505 - acc: 0.8937 - val_loss: 2.5143 - val_acc: 0.8319\n",
      "Epoch 10/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.2340 - acc: 0.9010 - val_loss: 2.5873 - val_acc: 0.8287\n",
      "Epoch 11/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.2220 - acc: 0.9073 - val_loss: 2.9343 - val_acc: 0.8086\n",
      "Epoch 12/12\n",
      "780/779 [==============================] - 37s 48ms/step - loss: 0.2097 - acc: 0.9118 - val_loss: 3.8083 - val_acc: 0.7491\n",
      "Epoch 1/12\n",
      "780/779 [==============================] - 80s 102ms/step - loss: 0.6732 - acc: 0.5767 - val_loss: 4.9864 - val_acc: 0.6313\n",
      "Epoch 2/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.5716 - acc: 0.7058 - val_loss: 6.5042 - val_acc: 0.5866\n",
      "Epoch 3/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.4754 - acc: 0.7707 - val_loss: 4.9000 - val_acc: 0.6824\n",
      "Epoch 4/12\n",
      "780/779 [==============================] - 80s 102ms/step - loss: 0.4077 - acc: 0.8169 - val_loss: 4.1971 - val_acc: 0.7158\n",
      "Epoch 5/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.3619 - acc: 0.8383 - val_loss: 3.2486 - val_acc: 0.7796\n",
      "Epoch 6/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.3253 - acc: 0.8560 - val_loss: 3.1133 - val_acc: 0.7960\n",
      "Epoch 7/12\n",
      "780/779 [==============================] - 79s 102ms/step - loss: 0.2944 - acc: 0.8727 - val_loss: 2.6042 - val_acc: 0.8267\n",
      "Epoch 8/12\n",
      "780/779 [==============================] - 80s 102ms/step - loss: 0.2706 - acc: 0.8830 - val_loss: 2.6875 - val_acc: 0.8210\n",
      "Epoch 9/12\n",
      "780/779 [==============================] - 79s 102ms/step - loss: 0.2485 - acc: 0.8944 - val_loss: 2.4863 - val_acc: 0.8391\n",
      "Epoch 10/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.2447 - acc: 0.8961 - val_loss: 4.2260 - val_acc: 0.7218\n",
      "Epoch 11/12\n",
      "780/779 [==============================] - 79s 101ms/step - loss: 0.2319 - acc: 0.9019 - val_loss: 2.8267 - val_acc: 0.8058\n",
      "Epoch 12/12\n",
      "780/779 [==============================] - 80s 102ms/step - loss: 0.2171 - acc: 0.9086 - val_loss: 2.4916 - val_acc: 0.8244\n"
     ]
    }
   ],
   "source": [
    "# Round two attemps\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "# NOTE: To shrink the VRAM fraction add this to ConfigProto\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "# K.tf.ConfigProto(gpu_options=gpu_options)\n",
    "\n",
    "cfg = K.tf.ConfigProto(device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import normalize\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, rotation_range=10, width_shift_range=0.1,\n",
    "                             height_shift_range=0.1, shear_range=0.15,\n",
    "                             zoom_range=0.1, channel_shift_range=10., horizontal_flip=True)\n",
    "\n",
    "X = pickle.load(open(\"data/pickled/X.pickle\", \"rb\"))\n",
    "y = pickle.load(open(\"data/pickled/y.pickle\", \"rb\"))\n",
    "\n",
    "# X = keras.utils.normalize(X, axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            K.clear_session()\n",
    "            NAME = \"{}-conv-{}-nodes{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            tensorboard = TensorBoard(log_dir='logs_2/{}'.format(NAME))\n",
    "            \n",
    "            model = Sequential()\n",
    "\n",
    "            # the 128 can be anything\n",
    "            # 3,3 is the window of the convolution (3 pixels X 3 pixels)\n",
    "            # X.shape[1:] is 128, 128, 1 based ont he fact that I transformed the images to 128X128 pixes\n",
    "            model.add(Conv2D(layer_size, (3,3), input_shape = X.shape[1:]))\n",
    "            model.add(Activation(\"relu\"))\n",
    "#             model.add(Dropout(0.2))\n",
    "            model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "            \n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(2*layer_size, (3,3)))\n",
    "                model.add(Activation(\"relu\"))\n",
    "#                 model.add(Dropout(0.5))\n",
    "                model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "            \n",
    "            model.add(Flatten())\n",
    "            for l in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation(\"relu\"))\n",
    "#                 model.add(Dropout(0.5))\n",
    "\n",
    "            # Output Layer\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                         optimizer='adam',\n",
    "                         metrics=['accuracy'])\n",
    "            \n",
    "#             model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val), callbacks=[tensorboard])\n",
    "            model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                               steps_per_epoch = len(X) / 32,\n",
    "                               epochs = 12,\n",
    "                               validation_data=(X_val, y_val),\n",
    "                               callbacks=[tensorboard])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
