{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obvious things to tweak\n",
    "\n",
    "\n",
    "Number of layers\n",
    "Nodes per layer\n",
    "Dense layer at the end or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4conv-256nodes-1dense-32batch-1540834190\n",
      "5conv-256nodes-1dense-32batch-1540834190\n",
      "6conv-256nodes-1dense-32batch-1540834190\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "dense_layers = [1]\n",
    "layer_sizes = [256]\n",
    "batch_sizes = [32]\n",
    "conv_layers = [4, 5, 6]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            for batch_size in batch_sizes:\n",
    "                NAME = \"{}conv-{}nodes-{}dense-{}batch-{}\".format(conv_layer, layer_size, dense_layer, batch_size, int(time.time()))\n",
    "                print(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4conv-256nodes-1dense-32batch-Dropout+BatchReg-1540834194\n",
      "Epoch 1/40\n",
      "780/779 [==============================] - 198s 254ms/step - loss: 0.5974 - acc: 0.6921 - val_loss: 0.6336 - val_acc: 0.6924\n",
      "Epoch 2/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.4255 - acc: 0.7990 - val_loss: 0.5337 - val_acc: 0.7703\n",
      "Epoch 3/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.3432 - acc: 0.8516 - val_loss: 0.4419 - val_acc: 0.8112\n",
      "Epoch 4/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.2853 - acc: 0.8760 - val_loss: 0.2863 - val_acc: 0.8786\n",
      "Epoch 5/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.2521 - acc: 0.8937 - val_loss: 0.3801 - val_acc: 0.8643\n",
      "Epoch 6/40\n",
      "780/779 [==============================] - 199s 256ms/step - loss: 0.2311 - acc: 0.9029 - val_loss: 1.2149 - val_acc: 0.6373\n",
      "Epoch 7/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.2209 - acc: 0.9081 - val_loss: 0.3451 - val_acc: 0.8699\n",
      "Epoch 8/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.2001 - acc: 0.9199 - val_loss: 0.2087 - val_acc: 0.9182\n",
      "Epoch 9/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.1855 - acc: 0.9255 - val_loss: 0.2145 - val_acc: 0.9190\n",
      "Epoch 10/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1804 - acc: 0.9275 - val_loss: 0.2054 - val_acc: 0.9196\n",
      "Epoch 11/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1737 - acc: 0.9282 - val_loss: 0.2082 - val_acc: 0.9156\n",
      "Epoch 12/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.1683 - acc: 0.9341 - val_loss: 0.2435 - val_acc: 0.9042\n",
      "Epoch 13/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1576 - acc: 0.9369 - val_loss: 0.2014 - val_acc: 0.9226\n",
      "Epoch 14/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1578 - acc: 0.9371 - val_loss: 0.2031 - val_acc: 0.9232\n",
      "Epoch 15/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.1491 - acc: 0.9413 - val_loss: 0.2307 - val_acc: 0.9128\n",
      "Epoch 16/40\n",
      "780/779 [==============================] - 193s 247ms/step - loss: 0.1411 - acc: 0.9442 - val_loss: 0.2569 - val_acc: 0.9112\n",
      "Epoch 17/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1354 - acc: 0.9453 - val_loss: 0.3139 - val_acc: 0.8900\n",
      "Epoch 18/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1395 - acc: 0.9441 - val_loss: 0.2161 - val_acc: 0.9220\n",
      "Epoch 19/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1250 - acc: 0.9493 - val_loss: 0.3244 - val_acc: 0.8876\n",
      "Epoch 20/40\n",
      "780/779 [==============================] - 193s 248ms/step - loss: 0.1299 - acc: 0.9489 - val_loss: 0.1893 - val_acc: 0.9345\n",
      "Epoch 21/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1220 - acc: 0.9517 - val_loss: 0.1727 - val_acc: 0.9307\n",
      "Epoch 22/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.1179 - acc: 0.9531 - val_loss: 0.1815 - val_acc: 0.9323\n",
      "Epoch 23/40\n",
      "780/779 [==============================] - 194s 248ms/step - loss: 0.1117 - acc: 0.9548 - val_loss: 0.1609 - val_acc: 0.9439\n",
      "Epoch 24/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.1129 - acc: 0.9555 - val_loss: 0.3174 - val_acc: 0.8996\n",
      "Epoch 25/40\n",
      "780/779 [==============================] - 195s 251ms/step - loss: 0.1157 - acc: 0.9537 - val_loss: 0.5670 - val_acc: 0.8521\n",
      "Epoch 26/40\n",
      "780/779 [==============================] - 194s 248ms/step - loss: 0.1079 - acc: 0.9571 - val_loss: 0.1442 - val_acc: 0.9421 a\n",
      "Epoch 27/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.1103 - acc: 0.9546 - val_loss: 0.1712 - val_acc: 0.9349\n",
      "Epoch 28/40\n",
      "780/779 [==============================] - 196s 252ms/step - loss: 0.1058 - acc: 0.9592 - val_loss: 0.2122 - val_acc: 0.9178\n",
      "Epoch 29/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.0994 - acc: 0.9608 - val_loss: 0.1513 - val_acc: 0.9451\n",
      "Epoch 30/40\n",
      "780/779 [==============================] - 193s 247ms/step - loss: 0.0978 - acc: 0.9601 - val_loss: 0.1666 - val_acc: 0.9407\n",
      "Epoch 31/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.0975 - acc: 0.9619 - val_loss: 0.1330 - val_acc: 0.9567\n",
      "Epoch 32/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.0957 - acc: 0.9634 - val_loss: 0.1937 - val_acc: 0.9228\n",
      "Epoch 33/40\n",
      "780/779 [==============================] - 195s 249ms/step - loss: 0.0937 - acc: 0.9646 - val_loss: 0.1897 - val_acc: 0.9345\n",
      "Epoch 34/40\n",
      "780/779 [==============================] - 197s 253ms/step - loss: 0.0899 - acc: 0.9657 - val_loss: 0.1561 - val_acc: 0.9501\n",
      "Epoch 35/40\n",
      "780/779 [==============================] - 194s 248ms/step - loss: 0.0904 - acc: 0.9649 - val_loss: 0.1340 - val_acc: 0.9525\n",
      "Epoch 36/40\n",
      "780/779 [==============================] - 196s 251ms/step - loss: 0.0849 - acc: 0.9670 - val_loss: 0.1422 - val_acc: 0.9517\n",
      "Epoch 37/40\n",
      "780/779 [==============================] - 192s 247ms/step - loss: 0.0867 - acc: 0.9671 - val_loss: 0.1466 - val_acc: 0.9523\n",
      "Epoch 38/40\n",
      "780/779 [==============================] - 193s 248ms/step - loss: 0.0831 - acc: 0.9678 - val_loss: 0.1977 - val_acc: 0.9331\n",
      "Epoch 39/40\n",
      "780/779 [==============================] - 196s 251ms/step - loss: 0.0833 - acc: 0.9685 - val_loss: 0.1662 - val_acc: 0.9389\n",
      "Epoch 40/40\n",
      "780/779 [==============================] - 193s 247ms/step - loss: 0.0798 - acc: 0.9686 - val_loss: 0.1465 - val_acc: 0.9479\n",
      "5conv-256nodes-1dense-32batch-Dropout+BatchReg-1540841981\n",
      "Epoch 1/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.6144 - acc: 0.6702 - val_loss: 1.0629 - val_acc: 0.5419\n",
      "Epoch 2/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.4465 - acc: 0.7904 - val_loss: 0.4308 - val_acc: 0.7948\n",
      "Epoch 3/40\n",
      "780/779 [==============================] - 193s 248ms/step - loss: 0.3407 - acc: 0.8505 - val_loss: 1.0360 - val_acc: 0.6094\n",
      "Epoch 4/40\n",
      "780/779 [==============================] - 195s 250ms/step - loss: 0.2843 - acc: 0.8788 - val_loss: 0.2671 - val_acc: 0.8850\n",
      "Epoch 5/40\n",
      "780/779 [==============================] - 194s 248ms/step - loss: 0.2605 - acc: 0.8884 - val_loss: 0.2634 - val_acc: 0.8850\n",
      "Epoch 6/40\n",
      "780/779 [==============================] - 193s 248ms/step - loss: 0.2279 - acc: 0.9055 - val_loss: 0.2385 - val_acc: 0.9030\n",
      "Epoch 7/40\n",
      "780/779 [==============================] - 195s 249ms/step - loss: 0.2081 - acc: 0.9136 - val_loss: 0.2128 - val_acc: 0.9102\n",
      "Epoch 8/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1955 - acc: 0.9187 - val_loss: 0.3281 - val_acc: 0.8585\n",
      "Epoch 9/40\n",
      "780/779 [==============================] - 192s 247ms/step - loss: 0.1793 - acc: 0.9270 - val_loss: 0.2121 - val_acc: 0.9180\n",
      "Epoch 10/40\n",
      "780/779 [==============================] - 193s 247ms/step - loss: 0.1758 - acc: 0.9290 - val_loss: 0.1581 - val_acc: 0.9313\n",
      "Epoch 11/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1724 - acc: 0.9288 - val_loss: 0.3580 - val_acc: 0.8415\n",
      "Epoch 12/40\n",
      "780/779 [==============================] - 193s 248ms/step - loss: 0.1539 - acc: 0.9384 - val_loss: 0.1715 - val_acc: 0.9248\n",
      "Epoch 13/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1503 - acc: 0.9403 - val_loss: 0.1982 - val_acc: 0.9210\n",
      "Epoch 14/40\n",
      "780/779 [==============================] - 194s 249ms/step - loss: 0.1462 - acc: 0.9422 - val_loss: 0.2758 - val_acc: 0.8960\n",
      "Epoch 15/40\n",
      "780/779 [==============================] - 192s 246ms/step - loss: 0.1363 - acc: 0.9442 - val_loss: 0.1692 - val_acc: 0.9361\n",
      "Epoch 16/40\n",
      "455/779 [================>.............] - ETA: 1:15 - loss: 0.1314 - acc: 0.9463"
     ]
    }
   ],
   "source": [
    "# Round two attemps\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "# NOTE: To shrink the VRAM fraction add this to ConfigProto\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "# K.tf.ConfigProto(gpu_options=gpu_options)\n",
    "\n",
    "cfg = K.tf.ConfigProto(device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.utils import normalize\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1,\n",
    "                             height_shift_range=0.1, shear_range=0.15,\n",
    "                             zoom_range=0.1, channel_shift_range=10., horizontal_flip=True)\n",
    "\n",
    "X = pickle.load(open(\"data/pickled/X.pickle\", \"rb\"))\n",
    "y = pickle.load(open(\"data/pickled/y.pickle\", \"rb\"))\n",
    "\n",
    "# X = keras.utils.normalize(X, axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            for batch_size in batch_sizes:\n",
    "                K.clear_session()\n",
    "                NAME = \"{}conv-{}nodes-{}dense-{}batch-Dropout+BatchReg-{}\".format(conv_layer, layer_size, dense_layer, batch_size, int(time.time()))\n",
    "                print(NAME)\n",
    "                tensorboard = TensorBoard(log_dir='logs_5/{}'.format(NAME))\n",
    "\n",
    "                model = Sequential()\n",
    "\n",
    "                # the 128 can be anything\n",
    "                # 3,3 is the window of the convolution (3 pixels X 3 pixels)\n",
    "                # X.shape[1:] is 128, 128, 1 based ont he fact that I transformed the images to 128X128 pixes\n",
    "                model.add(Conv2D(layer_size, (3,3), activation='relu', input_shape = X.shape[1:]))\n",
    "                model.add(BatchNormalization())\n",
    "#                 model.add(Activation(\"relu\"))\n",
    "#                 model.add(Dropout(0.2))\n",
    "                model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "                for l in range(conv_layer-1):\n",
    "                    model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "                    model.add(BatchNormalization())\n",
    "#                     model.add(Activation(\"relu\"))\n",
    "                    model.add(Dropout(0.1))\n",
    "                    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "                model.add(Flatten())\n",
    "                for l in range(dense_layer):\n",
    "                    model.add(Dense(layer_size))\n",
    "                    model.add(Activation(\"relu\"))\n",
    "                    model.add(Dropout(0.2))\n",
    "\n",
    "                # Output Layer\n",
    "                model.add(Dense(1))\n",
    "                model.add(Activation('sigmoid'))\n",
    "\n",
    "                model.compile(loss='binary_crossentropy',\n",
    "                             optimizer='adam',\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "    #             model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val), callbacks=[tensorboard])\n",
    "                model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                                   steps_per_epoch = len(X) / batch_size,\n",
    "                                   epochs = 40,\n",
    "                                   validation_data=(X_val, y_val),\n",
    "                                   callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil(1.25*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.25*64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
